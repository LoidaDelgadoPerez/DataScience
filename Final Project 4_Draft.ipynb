{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Technical notebook for Movielens engine recommendation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this delivery I will attempt to answer the question “What movie should I watch this evening?” by modelling MovieLens data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main variables are ratings, genres, movie titles and users.\n",
    "\n",
    "Ratings tend to be quite positive overall. Different variables other than the quality of movies could be impacting user's ratings.\n",
    "\n",
    "The most popular movie genres are Drama, Comedy, Action, Thriller, and Romance.\n",
    "The top five most watch movie titles are:\n",
    "\n",
    "American Beauty (1999) \n",
    "Jurassic Park (1993) \n",
    "Saving Private Ryan (1998) \n",
    "Matrix, The (1999) \n",
    "Back to the Future (1985)\n",
    "\n",
    "The demographic profile of our users is mainly males, aged 18 - 44 years old, working in a variety of occupations (e.g. students, professionals, academics and technicians) and located across US states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could be exploring different types of approaches for our movie recommendation as per James Lee blog post: \n",
    "https://medium.com/@james_aka_yale/the-4-recommendation-engines-that-can-predict-your-movie-tastes-bbec857b8223\n",
    "\n",
    "Types of Recommendation Engines we could explore as per James Lee blog post are:\n",
    "\n",
    "1. Content-Based\n",
    "\n",
    "Benefits:\n",
    "Easy to implement due to no training or optimization is involved.\n",
    "No need for data on other users, thus no cold-start or sparsity problems.\n",
    "Can recommend to users with unique tastes.\n",
    "Can recommend new & unpopular items.\n",
    "Can provide explanations for recommended items by listing content-features that caused an item to be recommended (in this case, movie genres)\n",
    "\n",
    "Drawbacks:\n",
    "Finding the appropriate features is hard.\n",
    "Does not recommend items outside a user’s content profile.\n",
    "Unable to exploit quality judgments of other users.\n",
    "Model performance decrease when having sparse data which hinders scalability of approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Memory-Based Collaborative Filtering\n",
    "\n",
    "Benefits:\n",
    "Easy to implement due to no training or optimization is involved\n",
    "Reasonable prediction quality.\n",
    "\n",
    "Drawbacks:\n",
    "It doesn't address the cold-start problem, that is when new user or new item enters the system.\n",
    "It can't deal with sparse data, meaning it's hard to find users that have rated the same items.\n",
    "It suffers when new users or items that don't have any ratings enter the system.\n",
    "It tends to recommend popular items.\n",
    "It doesn’t scale particularly well to massive datasets, especially for real-time recommendations based on user behavior similarities — which takes a lot of computations.\n",
    "Ratings matrices may be overfitting to noisy representations of user tastes and preferences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model-Based Collaborative Filtering (based on Matrix Factorization)\n",
    "\n",
    "The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items. James Less applied Dimensionality Reduction techniques to derive the tastes and preferences from the raw data and Singular Vector Decomposition (SVD)\n",
    "\n",
    "Why reduce dimensions?\n",
    "We can discover hidden correlations / features in the raw data.\n",
    "We can remove redundant and noisy features that are not useful.\n",
    "We can interpret and visualize the data easier.\n",
    "We can also access easier data storage and processing.\n",
    "\n",
    "Benefits:\n",
    "Widely used for recommender systems \n",
    "It deals better with scalability and sparsity than Memory-based CF \n",
    "   \n",
    "Drawbacks:\n",
    "Singular Vector Decomposition (SVD) is an outdates methodology and would be better using newer factorisation methods.\n",
    "Some of those could be PCA or Non-Negative Matrix Factorisation because they build on SVD. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Deep Learning / Neural Network\n",
    "\n",
    "Similar to that of Model-Based Matrix Factorization. The sparse matrix doesn't need to be orthogonal. \n",
    "We want our model to learn the values of embedding matrix itself. The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm \n",
    "\n",
    "Benefits:\n",
    "This model performed better than all the approaches James Lee attempted before. \n",
    "\n",
    "Drawbacks:\n",
    "High computer performance needed\n",
    "Important investment of time needed in tunning the model\n",
    "Highly complex model with difficulty for troubleshooting \n",
    "Last lesson of this course which means no practical knowledge of methodology as yet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After researching on what's the best approach for our model considering the project and data set constrains I decided to implement a recommendation engine that doesn't consider users ratings because the variable is not always available and I would like the model to be as replicable as possible in other scenarios. My aim is to make meaninful movie recommendations in this case with the least user information needed so we can reuse our model in other contexts (e.g. recommend unrated events on a different website)\n",
    "\n",
    "For this, I will attempt to use the Latent Dirichlet Allocation (LDA) modelling method.\n",
    "\n",
    "In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model. Source: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "My goal is grouping the movie titles watched into themes so recommendations are generated based on top movies within a specific theme that the user hasn't watched yet.\n",
    "\n",
    "Therefore, I expect my model to be similar to James Lee's Model-Based Collaborative Filtering but using a more sophisticated method as LDA is meant to be. I am also inpiring my project on the New York Times recommendation engine blog post located here:\n",
    "https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/?_r=0\n",
    "\n",
    "Benefits:\n",
    "Widely used for recommender systems. LDA is central to topic modeling and has really revolutionized the field\n",
    "LDA tends to perform well on small datasets because Bayesian methods can avoid overfitting the data\n",
    "It deals better with scalability and sparsity than Memory-based CF \n",
    "LDA is a probabilistic model with interpretable topics\n",
    "LDA gives you categories for free, in any data set\n",
    "\n",
    "Drawbacks:\n",
    "It's hard to know when LDA is working becasue themes are soft-clusters so there is no objective metric to say \"this is the best choice\" of hyperparameters\n",
    "Fixed K (the number of themes is fixed and must be known ahead of time)\n",
    "Uncorrelated topics (Dirichlet theme distribution cannot capture correlations)\n",
    "Non-hierarchical (in data-limited regimes hierarchical models allow sharing of data)\n",
    "Static (no evolution of themes over time)\n",
    "Bag of words (assumes words are exchangeable, sentence structure is not modeled)\n",
    "Unsupervised (sometimes weak supervision is desirable, e.g. in sentiment analysis)\n",
    "The accuracy of statistical inference (which is the base of LDA) depends on the number of observations.\n",
    "\n",
    "I will be using the \"Single Variable Strategy\" in my approach meaning that I will start with the most important variable and slowly add in while paying attention to the model's performance if scope allows for it. My variables of choice are Movie titles users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformations needed pre modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loida\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import gensim\n",
    "import spacy\n",
    "nlp_toolkit = spacy.load(\"en\")\n",
    "\n",
    "## Load spacy\n",
    "import spacy\n",
    "\n",
    "# Setting up spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "#Regular expressions\n",
    "import regex as re\n",
    "\n",
    "# Gensim is used for LDA (and other models)\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "# Setting up spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Users_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>2858</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>480</td>\n",
       "      <td>2672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>2028</td>\n",
       "      <td>2653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2571</td>\n",
       "      <td>2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1270</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID Users_count\n",
       "2651     2858        3428\n",
       "466       480        2672\n",
       "1848     2028        2653\n",
       "2374     2571        2590\n",
       "1178     1270        2583"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bringing in movie frequency data (unpickling my moviewatchedrank dataframe)\n",
    "unpickled_moviewatchedrank = pd.read_pickle('moviefreq.pkl')\n",
    "\n",
    "#Reassigning\n",
    "movie_freq = unpickled_moviewatchedrank\n",
    "movie_freq.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>Musical|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp                                   Title  \\\n",
       "0       1     1193       5  978300760  One Flew Over the Cuckoo's Nest (1975)   \n",
       "1       1      661       3  978302109        James and the Giant Peach (1996)   \n",
       "2       1      914       3  978301968                     My Fair Lady (1964)   \n",
       "3       1     3408       4  978300275                  Erin Brockovich (2000)   \n",
       "4       1     2355       5  978824291                    Bug's Life, A (1998)   \n",
       "\n",
       "                         Genres Gender  Age  Occupation Zip-code  \n",
       "0                         Drama      F    1          10    48067  \n",
       "1  Animation|Children's|Musical      F    1          10    48067  \n",
       "2               Musical|Romance      F    1          10    48067  \n",
       "3                         Drama      F    1          10    48067  \n",
       "4   Animation|Children's|Comedy      F    1          10    48067  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bringing in rest of data (unpickling my dfc dataframe)\n",
    "unpickled_dfc = pd.read_pickle('dfc.pkl')\n",
    "\n",
    "#Reassigning\n",
    "dfc = unpickled_dfc\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Users_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>2858</td>\n",
       "      <td>Misérables, Les (1998)</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>480</td>\n",
       "      <td>Superman (1978)</td>\n",
       "      <td>2672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>2028</td>\n",
       "      <td>Babes in Toyland (1961)</td>\n",
       "      <td>2653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2571</td>\n",
       "      <td>Brady Bunch Movie, The (1995)</td>\n",
       "      <td>2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1270</td>\n",
       "      <td>Four Weddings and a Funeral (1994)</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID                               Title Users_count\n",
       "2651     2858              Misérables, Les (1998)        3428\n",
       "466       480                     Superman (1978)        2672\n",
       "1848     2028             Babes in Toyland (1961)        2653\n",
       "2374     2571       Brady Bunch Movie, The (1995)        2590\n",
       "1178     1270  Four Weddings and a Funeral (1994)        2583"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Swapping MovieIDs (movie_freq) for Movie titles (dfc)\n",
    "movie_freq1 = movie_freq.join(dfc.Title, on='MovieID',how='inner', lsuffix='movie_freq', rsuffix='', sort=False)\n",
    "movie_freq1.head()\n",
    "\n",
    "#Re ordering my columns and sorting by users count descending\n",
    "titles_freq = movie_freq1.reindex (columns=['MovieID','Title', 'Users_count'])\n",
    "titles_freqc = titles_freq.sort_values('Users_count', ascending=False).head(20)\n",
    "titles_freqc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent varible modelling with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending\n",
    "\n",
    "_Latent variable models_ are different in that instead of attempting to recreate rules of language, we'll try to understand language based on **how** the words are used. For example, we won't attempt to learn that 'bad' and 'badly' are related because they share the same root, but instead we'll determine that they are related because they are often used in the same way often or near the same words.\n",
    "\n",
    "We'll use _unsupervised_ learning techniques (discovering patterns or structure) to extract the information.\n",
    "\n",
    "Rather than inferring that 'Python' and 'C++' are both programming languages because they are often a noun preceded by the verb 'program' or 'code', we'll infer a category by identifying that they are often used in the same way. We won't need to guide them with particular phrases to look for parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending\n",
    "\n",
    "_Latent variable models_ are models in which we assume that the data we are observing has some hidden, underlying structure that we can't see, and which we'd like to learn. These hidden, underlying structure are the _latent_ variables we want our model to understand.\n",
    "\n",
    "Text processing is a common application of latent variable models. Again, in the classical sense we know that language is built by a set of pre-structured grammar rules and vocabulary; however, we also we know that we break those rules pretty often and create new words that get added into our vocabulary (see: selfie).\n",
    "\n",
    "Instead of attempting to learn the rules of 'proper' grammar, we instead look to uncover the hidden structure and ignore preexisting rules (which might not even describe our syntax anyway). Sometimes, the hidden structure we uncover _are_ the basic rules of our language, but sometimes they may also unveil something new.\n",
    "\n",
    "These techniques are commonly used for recommending news articles or mining large troves of data data and trying to find commonalities. Topic modeling, a method we will discuss in today's class, is used in the [NY times recommendation engine](http://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/?_r=0). They attempt to map their articles to a latent space (or underlying structure) of topics using the content of the article.\n",
    "\n",
    "[Lyst](http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/), an online fashion retailer, uses latent representations of clothing descriptions to find similar clothing. If we can map phrases like 'chelsea boot' or 'felted hat' to some underlying structure, we can use that new structure to find similar products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending\n",
    "\n",
    "Our previous 'representation' of a set of text documents (articles) for classification was a matrix with one row per document and one column per word (or _n-gram_).\n",
    "\n",
    "![Word Factorization Matrix](./assets/images/word-matrix-factorization.png)\n",
    "\n",
    "While this does sum up most of the information, it does drop a few things - mostly structure and order. Additionally, many of the columns may be dependent on each other (or correlated).\n",
    "\n",
    "For example, an article that contains the word 'IPO' is also likely to contain the work 'stock' or 'NASDAQ'.  Therefore, those columns are repetitive and both of those columns likely represent the same 'concept' or idea. For classification, we may not care if the document has the word 'IPO' or 'NASDAQ' or 'stocks', but just that it has financial-related words.\n",
    "\n",
    "One way to do this is with regularization - `L1` or `lasso` regularization tends to remove repetitive features by bringing their learned coefficients to 0.\n",
    "\n",
    "Another is to perform `dimensionality reduction` - where we first identify the correlated columns and then replace them with a column that represents the concept they have in common. For instance, we could replace the 'IPO', 'stocks', and 'NASDAQ' column with a single - 'HasFinancialWords' column.\n",
    "\n",
    "There are many techniques to do this automatically and most follow a very similar approach:\n",
    "\n",
    "1. Identify correlated columns\n",
    "2. Replace them with a new column that encapsulates the others\n",
    "\n",
    "The techniques vary in how they define correlation and how much of the relationship between the original and new columns you need to save.\n",
    "\n",
    "There are many dimensionality techniques built into `scikit-learn`. One of the most common is **PCA** or **Principal Components Analysis**. Like most of the models we've seen, dimensionality techniques can vary between _linear_ or _non-linear_, meaning that they pick up linear or non-linear correlations between columns.\n",
    "\n",
    "**PCA** when applied to text data is sometimes known as **LSI** or **Latent Semantic Indexing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending\n",
    "\n",
    "Mixture models (and specifically **LDA** or **Latent Dirichlet Allocation**) take this concept further and generate more structure around the documents. Instead of just replacing correlated columns, we create clusters of common words and generate probability distributions to explicitly state how related words are.\n",
    "\n",
    "To understand this better, let's imagine a new way to generate text:\n",
    "\n",
    "1. Start writing a document\n",
    "    1. First choose a topic (sports, news, science)\n",
    "        1. Choose a word from that topic\n",
    "    2. Repeat\n",
    "2. Repeat for the next document\n",
    "\n",
    "What this 'model' of text is assuming is that each document is some _mixture_ of topics. It may be mostly science, but may contain some business information. The _latent_ structure we want to uncover are the topics (or concepts) that generated that text.\n",
    "\n",
    "_Latent Dirichlet Allocation_ is a model that assumes this is the way text is generated and then attempts to learn two things:\n",
    "\n",
    "    1. What is the _word distribution_ of each topic?\n",
    "    2. What is the _topic distribution_ of each document?\n",
    "    \n",
    "The _word distribution_ is a multinomial distribution for each topic representing what words are most likely from that topic.\n",
    "\n",
    "Let's say we have 3 topics: sports, business, science.\n",
    "For each topic, we uncover the words most likely to come from them:\n",
    "\n",
    "For each word and topic pair, we learn some `P ( word | topic) `\n",
    "\n",
    "The _topic distribution_ is a multinomial distribution for each document representing which topics are most likely to be in that document. For all documents, we then have a distribution over {sports, science, business}\n",
    "\n",
    "Topic models are useful for organizing a collection of documents and uncovering the main underlying concepts.\n",
    "\n",
    "There are many variants as well, that attempt to incorporate more structure into the 'model'\n",
    "\n",
    " - Supervised Topic Models\n",
    "    - Guide the process with pre-decided topics\n",
    " - Position-dependent topic models\n",
    "    - Ignore which words occur in what document but instead focus on _where_ they occur\n",
    " - Variable number of topics\n",
    "    - Test a different number of topics to find the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending\n",
    "\n",
    "- Latent variable models attempt to uncover structure from text.\n",
    "- Dimensionality reduction is focused on replacing correlated columns.\n",
    "- Topic modeling (or LDA) uncovers the topics that are most common to each document and then the words most common to those topics.\n",
    "- Word2Vec builds a representation of a word from the way it was used originally.\n",
    "- Both techniques avoid learning grammar rules and instead rely on large datasets. They learn based on how the words are used, making them very flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Users_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>2858</td>\n",
       "      <td>Misérables, Les (1998)</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>480</td>\n",
       "      <td>Superman (1978)</td>\n",
       "      <td>2672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>2028</td>\n",
       "      <td>Babes in Toyland (1961)</td>\n",
       "      <td>2653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2571</td>\n",
       "      <td>Brady Bunch Movie, The (1995)</td>\n",
       "      <td>2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1270</td>\n",
       "      <td>Four Weddings and a Funeral (1994)</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID                               Title Users_count\n",
       "2651     2858              Misérables, Les (1998)        3428\n",
       "466       480                     Superman (1978)        2672\n",
       "1848     2028             Babes in Toyland (1961)        2653\n",
       "2374     2571       Brady Bunch Movie, The (1995)        2590\n",
       "1178     1270  Four Weddings and a Funeral (1994)        2583"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_freqc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Misérables,', '(1998)', '(1998)', '(1998)', '(1978)', '(1978)', '(1978)', '(1961)', '(1961)', '(1961)', 'Movie,', '(1995)', '(1995)', '(1995)', '(1994)', '(1994)', '(1994)', '(1997)', '(1997)', '(1997)']\n"
     ]
    }
   ],
   "source": [
    "#Addint stop words to exclusion list\n",
    "\n",
    "def cust_stop_list(lists):\n",
    "    '''\n",
    "    Take a list of lists and looks for specific strings.\n",
    "    Adds to stop_list\n",
    "\n",
    "    '''\n",
    "\n",
    "    stop_list = []\n",
    "\n",
    "    for line in lists:\n",
    "        words = line.split(' ')\n",
    "        for word in words:\n",
    "            if '(' in word:\n",
    "                stop_list.append(word)\n",
    "            if ')' in word:\n",
    "                stop_list.append(word)\n",
    "            if ',' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1930' in word:\n",
    "                stop_list.append(word)  \n",
    "            if '1945' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1951' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1961' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1967' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1978' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1987' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1988' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1992' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1992' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1994' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1995' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1997' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1998' in word:\n",
    "                stop_list.append(word)\n",
    "            if '1999' in word:\n",
    "                stop_list.append(word)\n",
    "            if '2000' in word:\n",
    "                stop_list.append(word)\n",
    "    return(stop_list)\n",
    "\n",
    "#Conditionals on year should be done with a regex but can't seem to make it work atm\n",
    "\n",
    "stop_list = cust_stop_list(titles_freqc.Title)\n",
    "print(stop_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'de', 'i', 'thereupon', 'at', 'hers', 'never', 'eight', 'so', 'more', 'somehow', 'only', 'thus', 'put', 'alone', 'except', 'give', 'yet', 'over', 'although', 'often', 'seems', 'sometime', 'you', 'yourselves', 'nobody', 'hence', 'part', 'thin', 'please', 'we', 'whereby', 'their', 'show', 'ltd', 'whither', 'yours', 'empty', 'anywhere', 'its', 'without', 'now', 'very', 'been', 'do', 'am', 'onto', 'my', 'several', 'first', 'our', 'everything', 'next', 'become', 'former', 'seem', 'there', 'therein', 'her', 'off', 'whoever', 'beyond', 'fire', 'together', 'through', 'itself', 'whence', 'then', 'always', 'though', 'anything', 'eg', 'into', 'nowhere', 'sixty', 'this', 'down', 'thereby', 'among', 'may', 'seemed', 'who', 'indeed', 'mill', 'due', 'side', 'three', 'same', 'fill', 'cannot', 'if', 'me', 'his', 'inc', 'below', 'other', 'any', 'than', 'beside', 'above', 'an', 'call', 'for', 'how', 'she', 'whom', 'amoungst', 'eleven', 'whether', 'even', 'besides', 'being', 'elsewhere', 'whatever', 'as', 'the', 'must', 'amongst', 'it', 'fifteen', 'twelve', 'each', 'either', 'seeming', 'six', 'whereas', 'thick', 'him', 'back', 'until', 'about', 'toward', 'because', 'made', 'they', 'are', 'had', 'un', 'behind', 'during', 'herself', 're', 'was', 'mine', 'everyone', 'neither', 'can', 'he', 'cry', 'of', 'moreover', 'thru', 'thence', 'etc', 'ourselves', 'throughout', 'somewhere', 'both', 'anyhow', 'a', 'others', 'sometimes', 'whereafter', 'these', 'few', 'from', 'go', 'almost', 'already', 'keep', 'whose', 'will', 'every', 'per', 'under', 'many', 'something', 'con', 'no', 'see', 'where', 'ours', 'your', 'serious', 'rather', 'find', 'enough', 'detail', 'would', 'against', 'found', 'should', 'whereupon', 'also', 'twenty', 'here', 'mostly', 'or', 'themselves', 'herein', 'in', 'that', 'couldnt', 'hereafter', 'bill', 'wherein', 'were', 'ever', 'upon', 'namely', 'therefore', 'anyone', 'whole', 'describe', 'is', 'what', 'around', 'by', 'hereby', 'further', 'within', 'became', 'out', 'towards', 'well', 'much', 'some', 'ten', 'nothing', 'but', 'meanwhile', 'take', 'those', 'name', 'bottom', 'two', 'full', 'amount', 'forty', 'however', 'last', 'all', 'and', 'own', 'ie', 'less', 'beforehand', 'most', 'becomes', 'nevertheless', 'to', 'yourself', 'move', 'via', 'co', 'nine', 'along', 'before', 'hereupon', 'formerly', 'us', 'why', 'have', 'otherwise', 'not', 'everywhere', 'anyway', 'too', 'could', 'five', 'on', 'none', 'interest', 'done', 'someone', 'be', 'again', 'while', 'nor', 'third', 'becoming', 'afterwards', 'four', 'still', 'perhaps', 'else', 'after', 'system', 'one', 'them', 'thereafter', 'latterly', 'myself', 'whenever', 'which', 'hasnt', 'sincere', 'front', 'up', 'hundred', 'when', 'top', 'himself', 'might', 'least', 'another', 'between', 'latter', 'once', 'across', 'with', 'since', 'fifty', 'get', 'has', 'noone', 'cant', 'wherever', 'such'})\n"
     ]
    }
   ],
   "source": [
    "#Checking stop words list\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'de', 'i', 'hers', 'eight', 'so', 'more', 'except', 'yet', 'although', 'sometime', 'hence', 'part', 'thin', 'please', 'we', 'ltd', 'whither', 'its', 'without', 'very', 'been', 'am', 'my', 'several', 'there', 'our', 'former', 'whoever', 'her', 'together', 'itself', 'then', 'always', 'though', 'eg', 'nowhere', 'this', 'among', 'may', 'who', 'due', 'three', 'same', 'cannot', 'me', 'other', 'his', 'below', 'than', 'beside', 'above', 'how', 'she', 'amoungst', 'eleven', 'whether', 'even', 'being', 'whatever', 'as', 'the', 'must', 'fifteen', 'each', 'seeming', 'whereas', '(1930)', 'thick', 'back', 'because', 'made', 'they', 'un', 're', 'was', 'neither', 'can', 'cry', 'thru', 'moreover', 'etc', 'throughout', 'anyhow', 'a', 'others', 'sometimes', 'these', 'from', 'keep', 'almost', '(1945)', 'every', 'Hime)', 'con', 'no', 'where', 'ours', 'your', 'serious', 'rather', 'enough', 'detail', 'would', 'against', 'should', 'twenty', 'here', 'themselves', 'herein', 'in', 'that', 'hereafter', 'wherein', 'upon', 'namely', 'therefore', 'describe', 'what', 'around', 'by', 'hereby', 'further', 'within', 'became', 'out', 'those', 'some', 'ten', 'nothing', '(Mononoke', 'name', 'bottom', 'amount', 'to', 'ie', 'beforehand', 'nevertheless', 'becomes', 'move', 'co', 'nine', 'along', 'before', '(1995)', 'why', 'have', 'otherwise', 'not', 'everywhere', 'none', 'done', 'someone', 'be', 'while', 'afterwards', 'else', 'thereafter', 'them', 'one', 'latterly', 'whenever', '(1992)', 'sincere', 'front', 'when', 'Misérables,', 'might', 'least', 'with', 'across', 'fifty', 'get', 'noone', 'wherever', 'such', 'thereupon', 'at', 'never', 'somehow', 'thus', 'only', 'Untouchables,', 'put', 'alone', 'give', 'you', 'over', 'often', 'seems', 'yourselves', '(1994)', 'nobody', 'whereby', 'their', 'show', 'yours', 'empty', 'anywhere', 'now', 'do', 'onto', 'first', 'seem', 'everything', 'next', 'become', 'therein', 'Man,', 'off', 'beyond', 'fire', 'through', '(2000)', 'whence', 'anything', 'into', 'sixty', '(1961)', 'down', 'thereby', 'seemed', 'indeed', 'mill', 'side', 'fill', 'if', 'inc', 'any', '(1967)', 'whom', 'an', 'call', 'for', '(1978)', 'besides', 'elsewhere', 'amongst', 'it', 'twelve', 'either', 'six', 'him', '(1997)', 'until', 'about', 'toward', 'are', 'had', '(1999)', 'behind', 'during', 'herself', '(1998)', 'mine', 'everyone', 'he', 'of', 'thence', 'ourselves', 'somewhere', 'both', 'whereafter', 'few', 'go', 'whose', 'already', 'will', 'under', 'per', 'many', 'something', 'see', 'find', 'Mononoke,', 'shentan)', 'found', '(1988)', 'whereupon', 'also', 'mostly', 'or', 'couldnt', 'bill', '(1987)', 'ever', 'anyone', 'whole', 'is', 'Movie,', 'towards', 'well', 'much', 'two', 'but', 'meanwhile', 'take', '(1951)', 'full', 'however', 'were', 'forty', 'last', 'all', 'and', 'own', 'less', 'World,', 'most', 'yourself', 'via', 'hereupon', 'formerly', 'us', 'anyway', 'too', 'could', 'five', 'on', 'interest', 'again', 'nor', 'third', 'becoming', 'four', 'still', 'perhaps', 'after', 'system', 'which', 'myself', 'hasnt', 'up', 'hundred', 'top', '(Lashou', 'himself', 'another', 'between', 'latter', 'once', 'since', 'has', 'cant'})\n"
     ]
    }
   ],
   "source": [
    "#Adding custom stop words list\n",
    "\n",
    "cust_stop_words = stop_words.union(stop_list)\n",
    "print(cust_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1930',\n",
       " 1: '1945',\n",
       " 2: '1951',\n",
       " 3: '1961',\n",
       " 4: '1967',\n",
       " 5: '1978',\n",
       " 6: '1987',\n",
       " 7: '1988',\n",
       " 8: '1992',\n",
       " 9: '1994',\n",
       " 10: '1995',\n",
       " 11: '1997',\n",
       " 12: '1998',\n",
       " 13: '1999',\n",
       " 14: '2000',\n",
       " 15: 'act',\n",
       " 16: 'babes',\n",
       " 17: 'big',\n",
       " 18: 'boiled',\n",
       " 19: 'bonnie',\n",
       " 20: 'brady',\n",
       " 21: 'bunch',\n",
       " 22: 'clyde',\n",
       " 23: 'dracula',\n",
       " 24: 'election',\n",
       " 25: 'funeral',\n",
       " 26: 'hard',\n",
       " 27: 'heart',\n",
       " 28: 'hill',\n",
       " 29: 'hime',\n",
       " 30: 'house',\n",
       " 31: 'lashou',\n",
       " 32: 'les',\n",
       " 33: 'man',\n",
       " 34: 'misérables',\n",
       " 35: 'mononoke',\n",
       " 36: 'movie',\n",
       " 37: 'notting',\n",
       " 38: 'princess',\n",
       " 39: 'quiet',\n",
       " 40: 'running',\n",
       " 41: 'shentan',\n",
       " 42: 'sister',\n",
       " 43: 'strangers',\n",
       " 44: 'superman',\n",
       " 45: 'thing',\n",
       " 46: 'titanic',\n",
       " 47: 'toyland',\n",
       " 48: 'train',\n",
       " 49: 'untouchables',\n",
       " 50: 'weddings',\n",
       " 51: 'western',\n",
       " 52: 'world'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame({'titles': titles_freqc.Title})\n",
    "\n",
    "cv = CountVectorizer(binary = False, stop_words = cust_stop_words)\n",
    "\n",
    "docs = cv.fit_transform(df['titles'].dropna())\n",
    "\n",
    "#Build a mapping of numerical ID to word\n",
    "id2word = dict(enumerate(cv.get_feature_names()))\n",
    "\n",
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.133*\"1967\" + 0.133*\"clyde\" + 0.133*\"bonnie\" + 0.012*\"1987\" + 0.012*\"1999\" + 0.012*\"election\" + 0.012*\"1951\" + 0.012*\"heart\" + 0.012*\"1988\" + 0.012*\"1997\"'),\n",
       " (1,\n",
       "  '0.107*\"1998\" + 0.107*\"les\" + 0.107*\"misérables\" + 0.107*\"1999\" + 0.107*\"election\" + 0.010*\"1951\" + 0.010*\"1987\" + 0.010*\"1997\" + 0.010*\"heart\" + 0.010*\"1992\"'),\n",
       " (2,\n",
       "  '0.133*\"1999\" + 0.133*\"hill\" + 0.133*\"notting\" + 0.012*\"1987\" + 0.012*\"1951\" + 0.012*\"1997\" + 0.012*\"big\" + 0.012*\"superman\" + 0.012*\"1992\" + 0.012*\"titanic\"'),\n",
       " (3,\n",
       "  '0.133*\"world\" + 0.133*\"thing\" + 0.133*\"1951\" + 0.012*\"1987\" + 0.012*\"1999\" + 0.012*\"election\" + 0.012*\"1997\" + 0.012*\"big\" + 0.012*\"titanic\" + 0.012*\"superman\"'),\n",
       " (4,\n",
       "  '0.072*\"1992\" + 0.072*\"hard\" + 0.072*\"boiled\" + 0.072*\"shentan\" + 0.072*\"lashou\" + 0.072*\"toyland\" + 0.072*\"1961\" + 0.072*\"babes\" + 0.072*\"untouchables\" + 0.072*\"1987\"'),\n",
       " (5,\n",
       "  '0.103*\"mononoke\" + 0.103*\"1997\" + 0.054*\"princess\" + 0.054*\"hime\" + 0.054*\"funeral\" + 0.054*\"1951\" + 0.054*\"train\" + 0.054*\"1994\" + 0.054*\"strangers\" + 0.054*\"weddings\"'),\n",
       " (6,\n",
       "  '0.097*\"house\" + 0.097*\"sister\" + 0.097*\"1945\" + 0.097*\"1992\" + 0.097*\"act\" + 0.097*\"dracula\" + 0.009*\"1987\" + 0.009*\"1951\" + 0.009*\"1999\" + 0.009*\"1997\"'),\n",
       " (7,\n",
       "  '0.118*\"1978\" + 0.118*\"2000\" + 0.118*\"heart\" + 0.118*\"superman\" + 0.011*\"1987\" + 0.011*\"1999\" + 0.011*\"1997\" + 0.011*\"1951\" + 0.011*\"big\" + 0.011*\"1992\"'),\n",
       " (8,\n",
       "  '0.019*\"1999\" + 0.019*\"1987\" + 0.019*\"1951\" + 0.019*\"superman\" + 0.019*\"1997\" + 0.019*\"heart\" + 0.019*\"act\" + 0.019*\"1988\" + 0.019*\"untouchables\" + 0.019*\"election\"'),\n",
       " (9,\n",
       "  '0.072*\"1995\" + 0.072*\"movie\" + 0.072*\"man\" + 0.072*\"1987\" + 0.072*\"western\" + 0.072*\"brady\" + 0.072*\"quiet\" + 0.072*\"1930\" + 0.072*\"bunch\" + 0.072*\"running\"')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We convert our word-matrix into gensim's format\n",
    "corpus = Sparse2Corpus(docs, documents_columns = False)\n",
    "\n",
    "#Then we fit an LDA model. I will check for 10 topics for now because my movies are distributed across \n",
    "#5 different genres as per exploratory analysis findings and want to have an initial look at results\n",
    "lda_model = LdaModel(corpus = corpus, id2word = id2word, num_topics = 10)\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model above, we need to explicitly specify the number of topics we want the model to uncover. This is a critical step but unfortunately there is not a lot of guidance on the best way to select it. Having domain knowledge about your data may help.\n",
    "\n",
    "Once we have fit this model, like other unsupervised learning techniques, most of our validation techniques are mostly about interpretation.\n",
    "\n",
    "Did we learn reasonable topics?\n",
    "Do the words that make up a topic make sense?\n",
    "We can evaluate this by viewing the top words for each topic:\n",
    "\n",
    "gensim has a show_topics function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "(0, '0.133*\"1967\" + 0.133*\"clyde\" + 0.133*\"bonnie\" + 0.012*\"1987\" + 0.012*\"1999\"')\n",
      "\n",
      "Topic: 1\n",
      "(1, '0.107*\"1998\" + 0.107*\"les\" + 0.107*\"misérables\" + 0.107*\"1999\" + 0.107*\"election\"')\n",
      "\n",
      "Topic: 2\n",
      "(2, '0.133*\"1999\" + 0.133*\"hill\" + 0.133*\"notting\" + 0.012*\"1987\" + 0.012*\"1951\"')\n",
      "\n",
      "Topic: 3\n",
      "(3, '0.133*\"world\" + 0.133*\"thing\" + 0.133*\"1951\" + 0.012*\"1987\" + 0.012*\"1999\"')\n",
      "\n",
      "Topic: 4\n",
      "(4, '0.072*\"1992\" + 0.072*\"hard\" + 0.072*\"boiled\" + 0.072*\"shentan\" + 0.072*\"lashou\"')\n",
      "\n",
      "Topic: 5\n",
      "(5, '0.103*\"mononoke\" + 0.103*\"1997\" + 0.054*\"princess\" + 0.054*\"hime\" + 0.054*\"funeral\"')\n",
      "\n",
      "Topic: 6\n",
      "(6, '0.097*\"house\" + 0.097*\"sister\" + 0.097*\"1945\" + 0.097*\"1992\" + 0.097*\"act\"')\n",
      "\n",
      "Topic: 7\n",
      "(7, '0.118*\"1978\" + 0.118*\"2000\" + 0.118*\"heart\" + 0.118*\"superman\" + 0.011*\"1987\"')\n",
      "\n",
      "Topic: 8\n",
      "(8, '0.019*\"1999\" + 0.019*\"1987\" + 0.019*\"1951\" + 0.019*\"superman\" + 0.019*\"1997\"')\n",
      "\n",
      "Topic: 9\n",
      "(9, '0.072*\"1995\" + 0.072*\"movie\" + 0.072*\"man\" + 0.072*\"1987\" + 0.072*\"western\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A way of evaluating the model fit is by looking at the top words in each topic with gensim how_topics function\n",
    "#I expect some topics to represent some concepts clearly but others not so much.\n",
    "num_topics = 10\n",
    "num_words_per_topic = 5\n",
    "for ti, topic in enumerate(lda_model.show_topics(num_topics, num_words_per_topic)):\n",
    "    print(\"Topic: %d\" % (ti))\n",
    "    print (topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am having issues cleaning up the years from my titles so words within topics are more meaningful :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimisation and associated costs/benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
